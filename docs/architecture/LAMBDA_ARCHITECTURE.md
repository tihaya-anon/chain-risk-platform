# Lambda æ¶æ„è¯¦è§£ - æµæ‰¹ä¸€ä½“å¤„ç†

> Chain Risk Platform çš„ Lambda æ¶æ„è®¾è®¡ä¸å®ç°

---

## ğŸ“– ä»€ä¹ˆæ˜¯ Lambda æ¶æ„

Lambda æ¶æ„æ˜¯ä¸€ç§å¤§æ•°æ®å¤„ç†æ¶æ„ï¼Œé€šè¿‡ç»“åˆ**æ‰¹å¤„ç†**å’Œ**æµå¤„ç†**æ¥å®ç°ï¼š
- **å®æ—¶æ€§**ï¼šæµå¤„ç†æä¾›ç§’çº§å“åº”
- **å‡†ç¡®æ€§**ï¼šæ‰¹å¤„ç†ä¿è¯æ•°æ®æœ€ç»ˆä¸€è‡´æ€§

### æ ¸å¿ƒæ€æƒ³
```
åŸå§‹æ•°æ® â†’ æ‰¹å¤„ç†å±‚ï¼ˆå‡†ç¡®ä½†æ…¢ï¼‰â†’ æ‰¹è§†å›¾
         â†“
         æµå¤„ç†å±‚ï¼ˆå¿«é€Ÿä½†å¯èƒ½æœ‰é”™ï¼‰â†’ å®æ—¶è§†å›¾
         â†“
         æœåŠ¡å±‚ï¼ˆåˆå¹¶æ‰¹è§†å›¾ + å®æ—¶è§†å›¾ï¼‰â†’ æŸ¥è¯¢ç»“æœ
```

---

## ğŸ—ï¸ æœ¬é¡¹ç›®çš„ Lambda æ¶æ„

### æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Ingestion Layer                      â”‚
â”‚                  é“¾ä¸Šæ•°æ®é‡‡é›† (Go)                           â”‚
â”‚                           â†“                                  â”‚
â”‚                    Kafka Topics                              â”‚
â”‚                  - raw-blocks                                â”‚
â”‚                  - transfers                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                         â”‚
        â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Speed Layer     â”‚      â”‚  Batch Layer     â”‚
â”‚  (å®æ—¶æµå¤„ç†)    â”‚      â”‚  (æ‰¹å¤„ç†è¦†ç›–)    â”‚
â”‚                  â”‚      â”‚                  â”‚
â”‚  Flink Stream    â”‚      â”‚  Spark Batch     â”‚
â”‚                  â”‚      â”‚                  â”‚
â”‚  - å¿«é€Ÿè§£æ      â”‚      â”‚  - å®Œæ•´è§£æ      â”‚
â”‚  - åŒå†™æ•°æ®åº“    â”‚      â”‚  - è¦†ç›–ä¿®æ­£      â”‚
â”‚  - ç®€å•è§„åˆ™      â”‚      â”‚  - å¤æ‚æ¨¡å‹      â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
     â”‚         â”‚               â”‚         â”‚
     â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”˜         â”‚
     â”‚                 â”‚   â”‚             â”‚
     â–¼                 â–¼   â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL   â”‚  â”‚    Neo4j     â”‚  â”‚ Graph Engine â”‚
â”‚              â”‚  â”‚              â”‚  â”‚              â”‚
â”‚ source:      â”‚  â”‚ source:      â”‚  â”‚ - å¢é‡åˆ†æ   â”‚
â”‚ stream/batch â”‚  â”‚ stream/batch â”‚  â”‚ - æ‰¹é‡åˆ†æ   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                 â”‚                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Serving Layer   â”‚
                â”‚  (æœåŠ¡å±‚)        â”‚
                â”‚                  â”‚
                â”‚  - Query Service â”‚
                â”‚  - Risk Service  â”‚
                â”‚  - Alert Service â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ ä¸‰å±‚æ¶æ„è¯¦è§£

### 1ï¸âƒ£ Speed Layerï¼ˆå®æ—¶æµå¤„ç†å±‚ï¼‰

#### èŒè´£
æä¾›**ç§’çº§**æ•°æ®å¤„ç†ï¼Œå¿«é€Ÿå“åº”ç”¨æˆ·æŸ¥è¯¢

#### æŠ€æœ¯æ ˆ
- **Flink Stream Processor** (Java)
- **Kafka** (æ¶ˆæ¯é˜Ÿåˆ—)

#### æ•°æ®æµ
```
Kafka (raw-blocks)
    â†“
Flink Stream Processor
    â”œâ”€ è§£æ Transfer (Native + ERC20)
    â”œâ”€ ç®€å•æ•°æ®éªŒè¯
    â””â”€ å®æ—¶é£é™©è§„åˆ™ï¼ˆé»‘åå•æ£€æŸ¥ï¼‰
    â†“
åŒå†™ç­–ç•¥
â”œâ”€ PostgreSQL (source='stream')
â”‚   â””â”€ ç”¨äº Query Service OLTP æŸ¥è¯¢
â”‚
â””â”€ Neo4j (source='stream')
    â””â”€ ç”¨äº Graph Engine å®æ—¶å›¾åˆ†æ
    â†“
å‘é€åˆ° Kafka (transfers)
    â””â”€ è§¦å‘ Graph Engine å¢é‡åˆ†æ
```

#### å®ç°ç¤ºä¾‹

```java processing/stream-processor/src/main/java/com/chainrisk/stream/job/TransferExtractionJob.java
public class TransferExtractionJob {
    public void execute() {
        // 1. æ¶ˆè´¹ Kafka
        DataStream<RawBlockData> blockStream = env
            .addSource(new FlinkKafkaConsumer<>("raw-blocks", ...))
            .uid("kafka-source");
        
        // 2. è§£æ Transfer
        DataStream<Transfer> transfers = blockStream
            .flatMap(new TransferParser())  // å¿«é€Ÿè§£æ
            .uid("transfer-parser");
        
        // 3. åŒå†™ PostgreSQL
        transfers.addSink(JdbcSink.sink(
            "INSERT INTO transfers (...) VALUES (...) " +
            "ON CONFLICT (tx_hash) DO UPDATE SET " +
            "  source = 'stream', " +
            "  updated_at = NOW()",
            ...
        )).uid("postgres-sink");
        
        // 4. åŒå†™ Neo4j
        transfers.addSink(new Neo4jSink<Transfer>() {
            @Override
            public void invoke(Transfer t, Context context) {
                try (Session session = driver.session()) {
                    session.run(
                        "MERGE (from:Address {address: $from, network: $network}) " +
                        "ON CREATE SET from.first_seen = timestamp(), " +
                        "              from.risk_score = 0.0, " +
                        "              from.source = 'stream' " +
                        "MERGE (to:Address {address: $to, network: $network}) " +
                        "ON CREATE SET to.first_seen = timestamp(), " +
                        "              to.risk_score = 0.0, " +
                        "              to.source = 'stream' " +
                        "MERGE (from)-[r:TRANSFER {tx_hash: $txHash}]->(to) " +
                        "ON CREATE SET r.amount = $amount, " +
                        "              r.timestamp = $timestamp, " +
                        "              r.source = 'stream'",
                        parameters(...)
                    );
                }
            }
        }).uid("neo4j-sink");
        
        // 5. å‘é€åˆ° Kafka è§¦å‘ä¸‹æ¸¸
        transfers.addSink(new FlinkKafkaProducer<>("transfers", ...))
            .uid("kafka-producer");
    }
}
```

#### ç‰¹ç‚¹
- âœ… **å®æ—¶æ€§å¥½**ï¼šç§’çº§å»¶è¿Ÿ
- âœ… **ååé‡é«˜**ï¼šå¯å¤„ç†é«˜é¢‘äº¤æ˜“
- âš ï¸ **å¯èƒ½æœ‰é”™**ï¼šæ•°æ®ä¸¢å¤±ã€è§£æå¤±è´¥ã€åŒºå—é‡ç»„
- âš ï¸ **ç®€å•è§„åˆ™**ï¼šæ— æ³•è¿è¡Œå¤æ‚ ML æ¨¡å‹

---

### 2ï¸âƒ£ Batch Layerï¼ˆæ‰¹å¤„ç†å±‚ï¼‰+ Data Lake (Hudi)

#### èŒè´£
- **æ•°æ®å½’æ¡£**ï¼šPostgreSQL å†·æ•°æ®å½’æ¡£åˆ° Hudi
- **æ‰¹å¤„ç†ä¿®æ­£**ï¼šåœ¨ Hudi ä¸Šè¿›è¡Œ UPSERT è¦†ç›–ä¿®æ­£
- **å†å²å­˜å‚¨**ï¼šHudi ä½œä¸ºå…¨é‡å†å²æ•°æ®å­˜å‚¨

#### æŠ€æœ¯æ ˆ
- **Apache Hudi** (æ•°æ®æ¹–å­˜å‚¨)
- **Spark Batch Processor** (Scala)
- **å…¨èŠ‚ç‚¹ RPC** (æ•°æ®æº)

#### æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Speed Layer                             â”‚
â”‚                                                                 â”‚
â”‚  Kafka â†’ Flink Stream â†’ PostgreSQL (çƒ­æ•°æ®, 7å¤©) + Neo4j        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ æ¯æ—¥å½’æ¡£ (02:00)
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Data Lake Layer (Hudi)                     â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Hudi Tables                          â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â”‚  transfers (MOR)     addresses (MOR)    risk_scores    â”‚   â”‚
â”‚  â”‚  - å…¨é‡å†å²æ•°æ®       - å…¨é‡åœ°å€æ•°æ®      - å†å²è¯„åˆ†     â”‚   â”‚
â”‚  â”‚  - æŒ‰ network/dt åˆ†åŒº - æŒ‰ network åˆ†åŒº                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â†‘                                    â”‚
â”‚                            â”‚                                    â”‚
â”‚                   Spark Batch (03:00)                           â”‚
â”‚                   (å…¨èŠ‚ç‚¹ RPC â†’ UPSERT ä¿®æ­£)                    â”‚
â”‚                            â”‚                                    â”‚
â”‚                            â†“                                    â”‚
â”‚              è¿‘æœŸè¢«ä¿®æ­£æ•°æ®å›å†™ PostgreSQL (04:00)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### æ•°æ®æµ

```
1. å½’æ¡£ä»»åŠ¡ (æ¯æ—¥ 02:00)
   PostgreSQL (7å¤©å‰æ•°æ®) â†’ Hudi
   ç„¶åä» PostgreSQL åˆ é™¤å·²å½’æ¡£æ•°æ®

2. æ‰¹å¤„ç†ä¿®æ­£ (æ¯æ—¥ 03:00)
   å…¨èŠ‚ç‚¹ RPC â†’ Spark â†’ Hudi (UPSERT è¦†ç›–)

3. çƒ­æ•°æ®å›å†™ (æ¯æ—¥ 04:00)
   Hudi ä¸­è¿‘æœŸè¢«ä¿®æ­£çš„æ•°æ® â†’ PostgreSQL
```

#### Hudi è¡¨è®¾è®¡

```sql
-- transfers è¡¨ (MOR æ¨¡å¼)
CREATE TABLE hudi_transfers (
    tx_hash STRING,              -- recordKey
    block_number BIGINT,         -- precombineKey
    from_addr STRING,
    to_addr STRING,
    amount DECIMAL(38,0),
    token_address STRING,
    timestamp BIGINT,
    network STRING,
    source STRING,               -- 'stream' / 'batch'
    created_at TIMESTAMP,
    corrected_at TIMESTAMP
) USING hudi
PARTITIONED BY (network, dt)
OPTIONS (
    type = 'mor',
    primaryKey = 'tx_hash',
    preCombineField = 'block_number',
    hoodie.cleaner.commits.retained = 24,
    hoodie.keep.min.commits = 20,
    hoodie.keep.max.commits = 30
);
```

#### å®ç°ç¤ºä¾‹

##### 1. å†·æ•°æ®å½’æ¡£ä»»åŠ¡

```python
# processing/batch-processor/jobs/archive_to_hudi.py

def archive_cold_data():
    spark = SparkSession.builder \
        .appName("Archive Cold Data to Hudi") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .getOrCreate()
    
    seven_days_ago = int((datetime.now() - timedelta(days=7)).timestamp())
    
    # 1. ä» PostgreSQL è¯»å–å†·æ•°æ®
    cold_data = spark.read.format("jdbc") \
        .option("url", "jdbc:postgresql://postgres:5432/chainrisk") \
        .option("query", f"""
            SELECT *, DATE(to_timestamp(timestamp)) as dt 
            FROM transfers 
            WHERE timestamp < {seven_days_ago}
        """) \
        .option("user", "postgres") \
        .option("password", "postgres") \
        .load()
    
    if cold_data.count() == 0:
        print("No cold data to archive")
        return
    
    # 2. å†™å…¥ Hudi
    cold_data.write.format("hudi") \
        .option("hoodie.table.name", "transfers") \
        .option("hoodie.datasource.write.operation", "upsert") \
        .option("hoodie.datasource.write.recordkey.field", "tx_hash") \
        .option("hoodie.datasource.write.precombine.field", "block_number") \
        .option("hoodie.datasource.write.partitionpath.field", "network,dt") \
        .option("hoodie.upsert.shuffle.parallelism", 200) \
        .mode("append") \
        .save("s3://chainrisk-datalake/hudi/transfers")
    
    # 3. ä» PostgreSQL åˆ é™¤å·²å½’æ¡£æ•°æ®
    with psycopg2.connect("postgresql://postgres:postgres@postgres:5432/chainrisk") as conn:
        with conn.cursor() as cur:
            cur.execute(f"DELETE FROM transfers WHERE timestamp < {seven_days_ago}")
            print(f"Deleted {cur.rowcount} rows from PostgreSQL")
        conn.commit()
```

##### 2. æ‰¹å¤„ç†ä¿®æ­£ä»»åŠ¡

```scala
// processing/batch-processor/src/main/scala/com/chainrisk/batch/TransferCorrectionJob.scala

object TransferCorrectionJob {
    def main(args: Array[String]): Unit = {
        val spark = SparkSession.builder()
            .appName("Transfer Correction to Hudi")
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
            .getOrCreate()
        
        val yesterday = LocalDate.now().minusDays(1)
        val yesterdayStart = yesterday.atStartOfDay().toEpochSecond(ZoneOffset.UTC)
        val yesterdayEnd = yesterday.plusDays(1).atStartOfDay().toEpochSecond(ZoneOffset.UTC)
        
        // 1. ä»å…¨èŠ‚ç‚¹é‡æ–°æ‰«ææ˜¨å¤©çš„åŒºå—
        val correctedTransfers = spark.read
            .format("web3")
            .option("rpcUrl", sys.env("ETH_RPC_URL"))
            .option("startTimestamp", yesterdayStart)
            .option("endTimestamp", yesterdayEnd)
            .option("confirmations", 12)
            .load()
            .withColumn("source", lit("batch"))
            .withColumn("corrected_at", current_timestamp())
            .withColumn("dt", to_date(from_unixtime(col("timestamp"))))
        
        // 2. UPSERT åˆ° Hudi (è‡ªåŠ¨è¦†ç›– stream æ•°æ®)
        correctedTransfers.write.format("hudi")
            .option("hoodie.table.name", "transfers")
            .option("hoodie.datasource.write.operation", "upsert")
            .option("hoodie.datasource.write.recordkey.field", "tx_hash")
            .option("hoodie.datasource.write.precombine.field", "block_number")
            .option("hoodie.datasource.write.partitionpath.field", "network,dt")
            .mode("append")
            .save("s3://chainrisk-datalake/hudi/transfers")
        
        // 3. è¿‘æœŸè¢«ä¿®æ­£çš„æ•°æ®å›å†™ PostgreSQL
        val sevenDaysAgo = LocalDate.now().minusDays(7).atStartOfDay().toEpochSecond(ZoneOffset.UTC)
        val recentCorrected = correctedTransfers.filter(col("timestamp") >= sevenDaysAgo)
        
        if (recentCorrected.count() > 0) {
            recentCorrected.write.format("jdbc")
                .option("url", "jdbc:postgresql://postgres:5432/chainrisk")
                .option("dbtable", "transfers")
                .option("user", "postgres")
                .option("password", "postgres")
                .mode("append")
                .save()
            // Note: éœ€è¦é…ç½® ON CONFLICT DO UPDATE
        }
    }
}
```

#### è°ƒåº¦æ–¹æ¡ˆ

```yaml
# Airflow DAG
dag:
  name: daily_hudi_pipeline
  schedule: "0 2 * * *"
  
  tasks:
    - name: archive_cold_data
      type: spark_submit
      script: archive_to_hudi.py
      schedule: "0 2 * * *"  # 02:00
      resources:
        executor_memory: 4g
        num_executors: 5
      
    - name: transfer_correction
      type: spark_submit
      script: TransferCorrectionJob.scala
      schedule: "0 3 * * *"  # 03:00
      depends_on: [archive_cold_data]
      resources:
        executor_memory: 8g
        num_executors: 10
      
    - name: hudi_compaction
      type: spark_submit
      script: hudi_compaction.py
      schedule: "0 5 * * *"  # 05:00
      depends_on: [transfer_correction]
      
    - name: graph_analysis_batch
      type: http_trigger
      url: http://graph-engine:8084/admin/clustering/run
      schedule: "0 6 * * *"  # 06:00
      depends_on: [transfer_correction]
```

#### ç‰¹ç‚¹
- âœ… **å­˜å‚¨æˆæœ¬ä½**ï¼šå†·æ•°æ®åœ¨å¯¹è±¡å­˜å‚¨ (S3/MinIO)
- âœ… **æ‰¹å¤„ç†æ— é”**ï¼šUPSERT åœ¨ Hudi ä¸Šæ“ä½œï¼Œä¸å½±å“ PostgreSQL
- âœ… **Schema æ¼”è¿›**ï¼šHudi åŸç”Ÿæ”¯æŒ
- âœ… **Time Travel**ï¼šæ”¯æŒå†å²ç‰ˆæœ¬æŸ¥è¯¢ï¼ˆå®¡è®¡éœ€æ±‚ï¼‰
- âœ… **PostgreSQL ä½“ç§¯å¯æ§**ï¼šåªä¿ç•™ 7 å¤©çƒ­æ•°æ®

---

### 3ï¸âƒ£ Serving Layerï¼ˆæœåŠ¡å±‚ï¼‰

#### èŒè´£
- åˆå¹¶çƒ­æ•°æ® (PostgreSQL) å’Œå†·æ•°æ® (Hudi) æŸ¥è¯¢
- æä¾›ç»Ÿä¸€æŸ¥è¯¢æ¥å£

#### æŠ€æœ¯æ ˆ
- **Query Service** (Go) - æŸ¥è¯¢è·¯ç”±
- **Trino/Presto** - Hudi æŸ¥è¯¢å¼•æ“
- **Risk Service** (Python) - é£é™©è¯„åˆ†
- **Alert Service** (Go) - å‘Šè­¦æœåŠ¡
- **Graph Engine** (Java) - å›¾åˆ†æ

#### æŸ¥è¯¢è·¯ç”±ç­–ç•¥

```go
// services/query-service/internal/service/transfer_service.go

func (s *TransferService) GetTransfers(addr string, startTime, endTime int64) ([]Transfer, error) {
    sevenDaysAgo := time.Now().AddDate(0, 0, -7).Unix()
    
    if startTime >= sevenDaysAgo {
        // å…¨éƒ¨åœ¨çƒ­æ•°æ®èŒƒå›´ï¼ŒåªæŸ¥ PostgreSQL
        return s.repo.QueryPostgres(addr, startTime, endTime)
    } else if endTime < sevenDaysAgo {
        // å…¨éƒ¨æ˜¯å†·æ•°æ®ï¼ŒåªæŸ¥ Hudi (é€šè¿‡ Trino)
        return s.repo.QueryHudi(addr, startTime, endTime)
    } else {
        // è·¨è¶Šå†·çƒ­è¾¹ç•Œï¼Œåˆå¹¶æŸ¥è¯¢
        hotData, err := s.repo.QueryPostgres(addr, sevenDaysAgo, endTime)
        if err != nil {
            return nil, err
        }
        coldData, err := s.repo.QueryHudi(addr, startTime, sevenDaysAgo)
        if err != nil {
            return nil, err
        }
        return s.mergeAndDedupe(coldData, hotData), nil
    }
}

// Hudi æŸ¥è¯¢é€šè¿‡ Trino
func (r *TransferRepository) QueryHudi(addr string, startTime, endTime int64) ([]Transfer, error) {
    query := `
        SELECT tx_hash, block_number, from_addr, to_addr, amount, timestamp, source
        FROM hudi.chainrisk.transfers
        WHERE (from_addr = ? OR to_addr = ?)
          AND timestamp >= ? AND timestamp < ?
        ORDER BY timestamp DESC
    `
    return r.trinoClient.Query(query, addr, addr, startTime, endTime)
}
```

#### æ•°æ®æºé€‰æ‹©

| æŸ¥è¯¢ç±»å‹    | æ•°æ®æº            | è¯´æ˜               |
| ----------- | ----------------- | ------------------ |
| è¿‘ 7 å¤©äº¤æ˜“ | PostgreSQL        | ä½å»¶è¿Ÿï¼Œé«˜å¹¶å‘     |
| å†å²äº¤æ˜“    | Hudi (Trino)      | å¤§èŒƒå›´æ‰«æï¼Œæˆæœ¬ä½ |
| è·¨æ—¶é—´èŒƒå›´  | PostgreSQL + Hudi | åˆå¹¶å»é‡           |
| å›¾æŸ¥è¯¢      | Neo4j             | å…³ç³»åˆ†æ           |

#### æŸ¥è¯¢ç­–ç•¥

```go services/query-service/internal/service/transfer_service.go
func (s *TransferService) GetAddressTransfers(address string) ([]Transfer, error) {
    // æŸ¥è¯¢ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨ batch æ•°æ®ï¼Œå…¶æ¬¡ stream æ•°æ®
    transfers, err := s.repo.FindByAddress(address, TransferQuery{
        OrderBy: "CASE WHEN source = 'batch' THEN 0 ELSE 1 END, timestamp DESC",
        Limit:   100,
    })
    
    // æ ‡è®°æ•°æ®æ¥æº
    for i := range transfers {
        if transfers[i].Source == "stream" && transfers[i].CorrectedAt == nil {
            transfers[i].DataQuality = "realtime"  // å®æ—¶æ•°æ®ï¼Œå¯èƒ½æœ‰è¯¯
        } else {
            transfers[i].DataQuality = "verified"  // æ‰¹å¤„ç†éªŒè¯è¿‡çš„æ•°æ®
        }
    }
    
    return transfers, err
}
```

#### Graph Engine å¢é‡ + æ‰¹é‡åˆ†æ

```java processing/graph-engine/src/main/java/com/chainrisk/graph/service/impl/GraphSyncServiceImpl.java
@Service
@RequiredArgsConstructor
public class GraphSyncServiceImpl implements GraphSyncService {
    
    // âœ… å¢é‡åˆ†æï¼ˆKafka è§¦å‘ï¼‰
    @KafkaListener(topics = "transfers", groupId = "graph-engine")
    public void onNewTransfer(Transfer transfer) {
        log.debug("Received new transfer: {}", transfer.getTxHash());
        
        // 1. å¢é‡èšç±»
        if (shouldTriggerClustering(transfer)) {
            clusteringService.runIncrementalClustering(
                transfer.getFromAddr(), 
                transfer.getToAddr()
            );
        }
        
        // 2. å¢é‡æ ‡ç­¾ä¼ æ’­
        if (isHighRiskAddress(transfer.getFromAddr())) {
            tagPropagationService.propagateFromAddress(transfer.getFromAddr());
        }
    }
    
    // âœ… æ‰¹é‡åˆ†æï¼ˆå®šæ—¶ä»»åŠ¡ï¼‰
    @Scheduled(cron = "0 0 3 * * ?") // æ¯å¤©å‡Œæ™¨ 3 ç‚¹
    public void runBatchAnalysis() {
        log.info("Starting daily batch graph analysis");
        
        // 1. å…¨å›¾èšç±»
        clusteringService.runFullClustering();
        
        // 2. å…¨å›¾æ ‡ç­¾ä¼ æ’­
        tagPropagationService.propagateAllTags();
        
        // 3. PageRank
        graphAnalysisService.runPageRank();
        
        // 4. ç¤¾åŒºå‘ç°
        graphAnalysisService.runCommunityDetection();
    }
}
```

---

## ğŸ“Š æ•°æ®è¡¨è®¾è®¡

### PostgreSQL è¡¨ç»“æ„

```sql
-- transfers è¡¨ï¼ˆæ”¯æŒæµæ‰¹è¦†ç›–ï¼‰
CREATE TABLE transfers (
    tx_hash VARCHAR(66) PRIMARY KEY,
    block_number BIGINT NOT NULL,
    from_addr VARCHAR(42) NOT NULL,
    to_addr VARCHAR(42) NOT NULL,
    amount NUMERIC(78, 0) NOT NULL,
    token_address VARCHAR(42),
    timestamp BIGINT NOT NULL,
    network VARCHAR(20) NOT NULL DEFAULT 'ethereum',
    
    -- å…ƒæ•°æ®
    source VARCHAR(10) NOT NULL,  -- 'stream' æˆ– 'batch'
    created_at TIMESTAMP NOT NULL DEFAULT NOW(),
    corrected_at TIMESTAMP,  -- æ‰¹å¤„ç†è¦†ç›–æ—¶é—´
    
    INDEX idx_from_addr (from_addr),
    INDEX idx_to_addr (to_addr),
    INDEX idx_block_number (block_number),
    INDEX idx_source (source)
);

-- æŸ¥è¯¢ç¤ºä¾‹ï¼šä¼˜å…ˆä½¿ç”¨ batch æ•°æ®
SELECT * FROM transfers 
WHERE from_addr = '0x123...'
ORDER BY 
    CASE WHEN source = 'batch' THEN 0 ELSE 1 END,
    timestamp DESC
LIMIT 100;
```

### Neo4j å›¾ç»“æ„

```cypher
// Address èŠ‚ç‚¹
CREATE (a:Address {
    address: '0x123...',
    network: 'ethereum',
    risk_score: 0.5,
    tags: ['high_frequency', 'mixer_interaction'],
    source: 'stream',  // 'stream' æˆ– 'batch'
    first_seen: timestamp(),
    corrected_at: timestamp()  // æ‰¹å¤„ç†è¦†ç›–æ—¶é—´
})

// TRANSFER å…³ç³»
CREATE (from)-[r:TRANSFER {
    tx_hash: '0xabc...',
    amount: '1000000000000000000',
    timestamp: 1704240000,
    block_number: 18000000,
    source: 'stream',  // 'stream' æˆ– 'batch'
    corrected_at: timestamp()  // æ‰¹å¤„ç†è¦†ç›–æ—¶é—´
}]->(to)

// æŸ¥è¯¢ç¤ºä¾‹ï¼šä¼˜å…ˆä½¿ç”¨ batch æ•°æ®
MATCH (from:Address {address: $addr})-[r:TRANSFER]->(to)
RETURN from, r, to
ORDER BY 
    CASE WHEN r.source = 'batch' THEN 0 ELSE 1 END,
    r.timestamp DESC
LIMIT 100
```

---

## ğŸ¯ åº”ç”¨åœºæ™¯å¯¹æ¯”

### åœºæ™¯ 1: Transfer æ•°æ®æå–

| ç»´åº¦           | Flink æµå¤„ç†   | Spark æ‰¹å¤„ç†           |
| -------------- | -------------- | ---------------------- |
| **æ•°æ®æº**     | Kafka å®æ—¶æ¶ˆæ¯ | å…¨èŠ‚ç‚¹ RPC é‡æ–°æ‰«æ    |
| **è§£æé€»è¾‘**   | ç®€åŒ–ç‰ˆï¼ˆå¿«é€Ÿï¼‰ | å®Œæ•´ç‰ˆï¼ˆå¤„ç†å¤æ‚åˆçº¦ï¼‰ |
| **æ•°æ®å®Œæ•´æ€§** | å¯èƒ½ä¸¢å¤±       | ä¿è¯å®Œæ•´               |
| **åŒºå—é‡ç»„**   | æ— æ³•å¤„ç†       | ç­‰å¾…ç¡®è®¤åå¤„ç†         |
| **æ–°åˆçº¦æ”¯æŒ** | éœ€è¦é‡å¯æ›´æ–°   | å¯ä»¥å›å¡«å†å²æ•°æ®       |
| **å»¶è¿Ÿ**       | ç§’çº§           | T+1 å¤©                 |

**è¦†ç›–åŸå› **: æµå¤„ç†å¯èƒ½ä¸¢å¤±æ•°æ®ã€è§£æé”™è¯¯ã€åŒºå—é‡ç»„

---

### åœºæ™¯ 2: åœ°å€é£é™©è¯„åˆ†

| ç»´åº¦           | Flink æµå¤„ç†     | Spark æ‰¹å¤„ç†                 |
| -------------- | ---------------- | ---------------------------- |
| **ç‰¹å¾å·¥ç¨‹**   | çª—å£å†…ç®€å•ç‰¹å¾   | å…¨å±€å†å²ç‰¹å¾                 |
| **æ¨¡å‹å¤æ‚åº¦** | è½»é‡è§„åˆ™å¼•æ“     | å¤æ‚ ML æ¨¡å‹ï¼ˆXGBoostã€GNNï¼‰ |
| **è®¡ç®—èµ„æº**   | å—é™ï¼ˆå»¶è¿Ÿè¦æ±‚ï¼‰ | å……è¶³ï¼ˆå¯ç”¨å¤§é‡ CPU/GPUï¼‰     |
| **æ•°æ®å®Œæ•´æ€§** | å¯èƒ½ç¼ºå°‘å…³è”æ•°æ® | å¯ä»¥ JOIN æ‰€æœ‰å†å²è¡¨         |
| **å»¶è¿Ÿ**       | ç§’çº§             | T+1 å¤©                       |

**è¦†ç›–åŸå› **: æ‰¹å¤„ç†å¯ä»¥è®¡ç®—å…¨å±€ç‰¹å¾ã€ä½¿ç”¨å¤æ‚æ¨¡å‹

---

### åœºæ™¯ 3: åœ°å€èšç±»ä¸æ ‡ç­¾ä¼ æ’­

| ç»´åº¦           | Graph Engine å¢é‡      | Graph Engine æ‰¹é‡           |
| -------------- | ---------------------- | --------------------------- |
| **è§¦å‘æ–¹å¼**   | Kafka æ¶ˆæ¯è§¦å‘         | å®šæ—¶ä»»åŠ¡ï¼ˆæ¯æ—¥å‡Œæ™¨ï¼‰        |
| **åˆ†æèŒƒå›´**   | å±€éƒ¨å­å›¾               | å…¨å›¾                        |
| **ç®—æ³•å¤æ‚åº¦** | ç®€å•èšç±»ï¼ˆUnion-Findï¼‰ | PageRankã€Louvainã€ç¤¾åŒºå‘ç° |
| **è¿­ä»£è®¡ç®—**   | ä¸æ”¯æŒ                 | æ”¯æŒï¼ˆSpark GraphXï¼‰        |
| **å»¶è¿Ÿ**       | ç§’çº§                   | æ¯æ—¥                        |

**è¦†ç›–åŸå› **: æ‰¹é‡åˆ†æå¯ä»¥è¿è¡Œå¤æ‚å›¾ç®—æ³•ã€è·å¾—å…¨å±€è§†å›¾

---

## ğŸ“ˆ ç›‘æ§ä¸æ•°æ®è´¨é‡

### æ•°æ®å·®å¼‚æŠ¥å‘Š

```sql
-- æ¯æ—¥æµæ‰¹æ•°æ®å·®å¼‚ç»Ÿè®¡
SELECT 
    DATE(created_at) as date,
    COUNT(*) FILTER (WHERE source = 'stream') as stream_count,
    COUNT(*) FILTER (WHERE source = 'batch') as batch_count,
    COUNT(*) FILTER (WHERE corrected_at IS NOT NULL) as corrected_count,
    ROUND(100.0 * COUNT(*) FILTER (WHERE corrected_at IS NOT NULL) / 
          NULLIF(COUNT(*) FILTER (WHERE source = 'stream'), 0), 2) as correction_rate_pct
FROM transfers
WHERE created_at >= CURRENT_DATE - INTERVAL '30 days'
GROUP BY DATE(created_at)
ORDER BY date DESC;
```

### ç›‘æ§æŒ‡æ ‡

```yaml
metrics:
  # æµå¤„ç†æŒ‡æ ‡
  flink_stream:
    - kafka_lag: æ¶ˆæ¯ç§¯å‹é‡
    - processing_rate: å¤„ç†é€Ÿç‡ï¼ˆæ¡/ç§’ï¼‰
    - error_rate: è§£æé”™è¯¯ç‡
    - neo4j_write_latency: Neo4j å†™å…¥å»¶è¿Ÿ
    
  # æ‰¹å¤„ç†æŒ‡æ ‡
  spark_batch:
    - job_duration: ä»»åŠ¡æ‰§è¡Œæ—¶é•¿
    - records_processed: å¤„ç†è®°å½•æ•°
    - correction_count: ä¿®æ­£è®°å½•æ•°
    - correction_rate: ä¿®æ­£ç‡
    
  # å›¾åˆ†ææŒ‡æ ‡
  graph_engine:
    - incremental_analysis_latency: å¢é‡åˆ†æå»¶è¿Ÿ
    - batch_analysis_duration: æ‰¹é‡åˆ†ææ—¶é•¿
    - clusters_created: åˆ›å»ºçš„èšç±»æ•°
    - tags_propagated: ä¼ æ’­çš„æ ‡ç­¾æ•°
```

---

## ğŸš€ ä¼˜åŠ¿æ€»ç»“

| ç»´åº¦           | Lambda æ¶æ„ä¼˜åŠ¿                             |
| -------------- | ------------------------------------------- |
| **å®æ—¶æ€§**     | Flink ç›´æ¥å†™å…¥ Neo4jï¼ŒGraph Engine ç§’çº§å“åº” |
| **å‡†ç¡®æ€§**     | Spark æ‰¹å¤„ç†è¦†ç›–ä¿®æ­£é”™è¯¯æ•°æ®                |
| **æ•°æ®å®Œæ•´æ€§** | æ‰¹å¤„ç†ä¿è¯æœ€ç»ˆä¸€è‡´æ€§                        |
| **ç³»ç»Ÿè§£è€¦**   | æµæ‰¹åˆ†ç¦»ï¼ŒGraph Engine æ— éœ€åŒæ­¥æ•°æ®         |
| **èµ„æºä¼˜åŒ–**   | å‡å°‘ PostgreSQL æŸ¥è¯¢å‹åŠ›ï¼Œæ— é‡å¤è®¡ç®—        |
| **å¯æ‰©å±•æ€§**   | æµæ‰¹ç‹¬ç«‹æ‰©å±•ï¼Œäº’ä¸å½±å“                      |

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [é¡¹ç›®æ€»è§ˆ](./PROJECT_OVERVIEW.md)
- [æŠ€æœ¯å†³ç­–è®°å½•](./TECH_DECISIONS.md)
- [å¼€å‘è®¡åˆ’](../development/DEVELOPMENT_PLAN.md)

---

**æœ€åæ›´æ–°**: 2026-01-03
