# Lambda æ¶æ„è¯¦è§£ - æµæ‰¹ä¸€ä½“å¤„ç†

> Chain Risk Platform çš„ Lambda æ¶æ„è®¾è®¡ä¸å®ç°

---

## ğŸ“– ä»€ä¹ˆæ˜¯ Lambda æ¶æ„

Lambda æ¶æ„æ˜¯ä¸€ç§å¤§æ•°æ®å¤„ç†æ¶æ„ï¼Œé€šè¿‡ç»“åˆ**æ‰¹å¤„ç†**å’Œ**æµå¤„ç†**æ¥å®ç°ï¼š
- **å®æ—¶æ€§**ï¼šæµå¤„ç†æä¾›ç§’çº§å“åº”
- **å‡†ç¡®æ€§**ï¼šæ‰¹å¤„ç†ä¿è¯æ•°æ®æœ€ç»ˆä¸€è‡´æ€§

### æ ¸å¿ƒæ€æƒ³
```
åŸå§‹æ•°æ® â†’ æ‰¹å¤„ç†å±‚ï¼ˆå‡†ç¡®ä½†æ…¢ï¼‰â†’ æ‰¹è§†å›¾
         â†“
         æµå¤„ç†å±‚ï¼ˆå¿«é€Ÿä½†å¯èƒ½æœ‰é”™ï¼‰â†’ å®æ—¶è§†å›¾
         â†“
         æœåŠ¡å±‚ï¼ˆåˆå¹¶æ‰¹è§†å›¾ + å®æ—¶è§†å›¾ï¼‰â†’ æŸ¥è¯¢ç»“æœ
```

---

## ğŸ—ï¸ æœ¬é¡¹ç›®çš„ Lambda æ¶æ„

### æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Ingestion Layer                      â”‚
â”‚                  é“¾ä¸Šæ•°æ®é‡‡é›† (Go)                           â”‚
â”‚                           â†“                                  â”‚
â”‚                    Kafka Topics                              â”‚
â”‚                  - raw-blocks                                â”‚
â”‚                  - transfers                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                         â”‚
        â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Speed Layer     â”‚      â”‚  Batch Layer     â”‚
â”‚  (å®æ—¶æµå¤„ç†)    â”‚      â”‚  (æ‰¹å¤„ç†è¦†ç›–)    â”‚
â”‚                  â”‚      â”‚                  â”‚
â”‚  Flink Stream    â”‚      â”‚  Spark Batch     â”‚
â”‚                  â”‚      â”‚                  â”‚
â”‚  - å¿«é€Ÿè§£æ      â”‚      â”‚  - å®Œæ•´è§£æ      â”‚
â”‚  - åŒå†™æ•°æ®åº“    â”‚      â”‚  - è¦†ç›–ä¿®æ­£      â”‚
â”‚  - ç®€å•è§„åˆ™      â”‚      â”‚  - å¤æ‚æ¨¡å‹      â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
     â”‚         â”‚               â”‚         â”‚
     â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”˜         â”‚
     â”‚                 â”‚   â”‚             â”‚
     â–¼                 â–¼   â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL   â”‚  â”‚    Neo4j     â”‚  â”‚ Graph Engine â”‚
â”‚              â”‚  â”‚              â”‚  â”‚              â”‚
â”‚ source:      â”‚  â”‚ source:      â”‚  â”‚ - å¢é‡åˆ†æ   â”‚
â”‚ stream/batch â”‚  â”‚ stream/batch â”‚  â”‚ - æ‰¹é‡åˆ†æ   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                 â”‚                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Serving Layer   â”‚
                â”‚  (æœåŠ¡å±‚)        â”‚
                â”‚                  â”‚
                â”‚  - Query Service â”‚
                â”‚  - Risk Service  â”‚
                â”‚  - Alert Service â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ ä¸‰å±‚æ¶æ„è¯¦è§£

### 1ï¸âƒ£ Speed Layerï¼ˆå®æ—¶æµå¤„ç†å±‚ï¼‰

#### èŒè´£
æä¾›**ç§’çº§**æ•°æ®å¤„ç†ï¼Œå¿«é€Ÿå“åº”ç”¨æˆ·æŸ¥è¯¢

#### æŠ€æœ¯æ ˆ
- **Flink Stream Processor** (Java)
- **Kafka** (æ¶ˆæ¯é˜Ÿåˆ—)

#### æ•°æ®æµ
```
Kafka (raw-blocks)
    â†“
Flink Stream Processor
    â”œâ”€ è§£æ Transfer (Native + ERC20)
    â”œâ”€ ç®€å•æ•°æ®éªŒè¯
    â””â”€ å®æ—¶é£é™©è§„åˆ™ï¼ˆé»‘åå•æ£€æŸ¥ï¼‰
    â†“
åŒå†™ç­–ç•¥
â”œâ”€ PostgreSQL (source='stream')
â”‚   â””â”€ ç”¨äº Query Service OLTP æŸ¥è¯¢
â”‚
â””â”€ Neo4j (source='stream')
    â””â”€ ç”¨äº Graph Engine å®æ—¶å›¾åˆ†æ
    â†“
å‘é€åˆ° Kafka (transfers)
    â””â”€ è§¦å‘ Graph Engine å¢é‡åˆ†æ
```

#### å®ç°ç¤ºä¾‹

```java processing/stream-processor/src/main/java/com/chainrisk/stream/job/TransferExtractionJob.java
public class TransferExtractionJob {
    public void execute() {
        // 1. æ¶ˆè´¹ Kafka
        DataStream<RawBlockData> blockStream = env
            .addSource(new FlinkKafkaConsumer<>("raw-blocks", ...))
            .uid("kafka-source");
        
        // 2. è§£æ Transfer
        DataStream<Transfer> transfers = blockStream
            .flatMap(new TransferParser())  // å¿«é€Ÿè§£æ
            .uid("transfer-parser");
        
        // 3. åŒå†™ PostgreSQL
        transfers.addSink(JdbcSink.sink(
            "INSERT INTO transfers (...) VALUES (...) " +
            "ON CONFLICT (tx_hash) DO UPDATE SET " +
            "  source = 'stream', " +
            "  updated_at = NOW()",
            ...
        )).uid("postgres-sink");
        
        // 4. åŒå†™ Neo4j
        transfers.addSink(new Neo4jSink<Transfer>() {
            @Override
            public void invoke(Transfer t, Context context) {
                try (Session session = driver.session()) {
                    session.run(
                        "MERGE (from:Address {address: $from, network: $network}) " +
                        "ON CREATE SET from.first_seen = timestamp(), " +
                        "              from.risk_score = 0.0, " +
                        "              from.source = 'stream' " +
                        "MERGE (to:Address {address: $to, network: $network}) " +
                        "ON CREATE SET to.first_seen = timestamp(), " +
                        "              to.risk_score = 0.0, " +
                        "              to.source = 'stream' " +
                        "MERGE (from)-[r:TRANSFER {tx_hash: $txHash}]->(to) " +
                        "ON CREATE SET r.amount = $amount, " +
                        "              r.timestamp = $timestamp, " +
                        "              r.source = 'stream'",
                        parameters(...)
                    );
                }
            }
        }).uid("neo4j-sink");
        
        // 5. å‘é€åˆ° Kafka è§¦å‘ä¸‹æ¸¸
        transfers.addSink(new FlinkKafkaProducer<>("transfers", ...))
            .uid("kafka-producer");
    }
}
```

#### ç‰¹ç‚¹
- âœ… **å®æ—¶æ€§å¥½**ï¼šç§’çº§å»¶è¿Ÿ
- âœ… **ååé‡é«˜**ï¼šå¯å¤„ç†é«˜é¢‘äº¤æ˜“
- âš ï¸ **å¯èƒ½æœ‰é”™**ï¼šæ•°æ®ä¸¢å¤±ã€è§£æå¤±è´¥ã€åŒºå—é‡ç»„
- âš ï¸ **ç®€å•è§„åˆ™**ï¼šæ— æ³•è¿è¡Œå¤æ‚ ML æ¨¡å‹

---

### 2ï¸âƒ£ Batch Layerï¼ˆæ‰¹å¤„ç†å±‚ï¼‰

#### èŒè´£
æä¾›**å‡†ç¡®**çš„æ•°æ®å¤„ç†ï¼Œè¦†ç›–ä¿®æ­£æµå¤„ç†çš„é”™è¯¯

#### æŠ€æœ¯æ ˆ
- **Spark Batch Processor** (Scala)
- **å…¨èŠ‚ç‚¹ RPC** (æ•°æ®æº)

#### æ•°æ®æµ
```
å…¨èŠ‚ç‚¹ RPC (é‡æ–°æ‰«ææ˜¨å¤©çš„åŒºå—)
    â†“
Spark Batch Processor
    â”œâ”€ å®Œæ•´è§£æé€»è¾‘ï¼ˆå¤„ç†å¤æ‚åˆçº¦ï¼‰
    â”œâ”€ ç­‰å¾…åŒºå—ç¡®è®¤ï¼ˆé¿å…é‡ç»„ï¼‰
    â”œâ”€ æ–°åˆçº¦ç±»å‹æ”¯æŒ
    â””â”€ å…¨å±€ç‰¹å¾è®¡ç®—
    â†“
è¦†ç›–å†™å…¥ç­–ç•¥
â”œâ”€ PostgreSQL (source='batch', è¦†ç›– stream)
â”‚   â””â”€ ON CONFLICT DO UPDATE SET source='batch', corrected_at=NOW()
â”‚
â””â”€ Neo4j (source='batch', è¦†ç›– stream)
    â””â”€ ON MATCH SET source='batch', corrected_at=timestamp()
    â†“
è§¦å‘ Graph Engine æ‰¹é‡åˆ†æ
    â””â”€ å…¨å›¾èšç±»ã€PageRankã€ç¤¾åŒºå‘ç°
```

#### å®ç°ç¤ºä¾‹

```scala processing/batch-processor/src/main/scala/TransferCorrectionJob.scala
object TransferCorrectionJob {
    def main(args: Array[String]): Unit = {
        val spark = SparkSession.builder()
            .appName("Transfer Correction")
            .getOrCreate()
        
        // 1. ä»å…¨èŠ‚ç‚¹é‡æ–°è·å–æ˜¨å¤©çš„åŒºå—
        val blocks = spark.read
            .format("web3")  // è‡ªå®šä¹‰ DataSource
            .option("rpcUrl", "https://eth-mainnet.g.alchemy.com/v2/...")
            .option("startBlock", yesterdayStart)
            .option("endBlock", yesterdayEnd)
            .option("confirmations", 12)  // ç­‰å¾… 12 ä¸ªç¡®è®¤
            .load()
        
        // 2. å®Œæ•´è§£æé€»è¾‘
        val transfers = blocks
            .flatMap(parseTransfersWithFullLogic)  // å¤„ç†å¤æ‚åˆçº¦
            .withColumn("source", lit("batch"))
            .withColumn("corrected_at", current_timestamp())
        
        // 3. è¦†ç›–å†™å…¥ PostgreSQL
        transfers.write
            .format("jdbc")
            .option("url", "jdbc:postgresql://...")
            .option("dbtable", "transfers")
            .option("conflictAction", 
                "ON CONFLICT (tx_hash) DO UPDATE SET " +
                "  from_addr = EXCLUDED.from_addr, " +
                "  to_addr = EXCLUDED.to_addr, " +
                "  amount = EXCLUDED.amount, " +
                "  source = 'batch', " +
                "  corrected_at = NOW()")
            .mode("append")
            .save()
        
        // 4. è¦†ç›–å†™å…¥ Neo4j
        transfers.foreachPartition { partition =>
            val driver = GraphDatabase.driver("bolt://neo4j:7687", ...)
            val session = driver.session()
            
            partition.foreach { row =>
                session.run(
                    "MERGE (from:Address {address: $from}) " +
                    "ON MATCH SET from.source = 'batch', " +
                    "             from.corrected_at = timestamp() " +
                    "MERGE (to:Address {address: $to}) " +
                    "ON MATCH SET to.source = 'batch', " +
                    "             to.corrected_at = timestamp() " +
                    "MERGE (from)-[r:TRANSFER {tx_hash: $txHash}]->(to) " +
                    "ON MATCH SET r.source = 'batch', " +
                    "             r.corrected_at = timestamp()",
                    parameters(...)
                )
            }
            
            session.close()
            driver.close()
        }
        
        // 5. è§¦å‘ Graph Engine æ‰¹é‡åˆ†æ
        triggerGraphAnalysis()
    }
}
```

#### è°ƒåº¦æ–¹æ¡ˆ

```yaml
# Airflow DAG
dag:
  name: daily_batch_correction
  schedule: "0 2 * * *"  # æ¯å¤©å‡Œæ™¨ 2 ç‚¹
  
  tasks:
    - name: transfer_correction
      type: spark_submit
      script: TransferCorrectionJob.scala
      resources:
        executor_memory: 4g
        executor_cores: 2
        num_executors: 10
      
    - name: risk_scoring_batch
      type: spark_submit
      script: RiskScoringBatchJob.py
      depends_on: [transfer_correction]
      
    - name: graph_analysis_batch
      type: http_trigger
      url: http://graph-engine:8084/admin/clustering/run
      depends_on: [transfer_correction]
```

#### ç‰¹ç‚¹
- âœ… **å‡†ç¡®æ€§é«˜**ï¼šå®Œæ•´è§£æé€»è¾‘
- âœ… **æ•°æ®å®Œæ•´**ï¼šä»å…¨èŠ‚ç‚¹é‡æ–°æ‰«æ
- âœ… **å¤„ç†å¤æ‚åœºæ™¯**ï¼šé‡ç»„ã€æ–°åˆçº¦ã€å¤æ‚ ML æ¨¡å‹
- âš ï¸ **å»¶è¿Ÿé«˜**ï¼šT+1 å¤©

---

### 3ï¸âƒ£ Serving Layerï¼ˆæœåŠ¡å±‚ï¼‰

#### èŒè´£
åˆå¹¶æ‰¹è§†å›¾å’Œå®æ—¶è§†å›¾ï¼Œæä¾›ç»Ÿä¸€æŸ¥è¯¢æ¥å£

#### æŠ€æœ¯æ ˆ
- **Query Service** (Go) - æŸ¥è¯¢ PostgreSQL
- **Risk Service** (Python) - é£é™©è¯„åˆ†
- **Alert Service** (Go) - å‘Šè­¦æœåŠ¡
- **Graph Engine** (Java) - å›¾åˆ†æ

#### æŸ¥è¯¢ç­–ç•¥

```go services/query-service/internal/service/transfer_service.go
func (s *TransferService) GetAddressTransfers(address string) ([]Transfer, error) {
    // æŸ¥è¯¢ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨ batch æ•°æ®ï¼Œå…¶æ¬¡ stream æ•°æ®
    transfers, err := s.repo.FindByAddress(address, TransferQuery{
        OrderBy: "CASE WHEN source = 'batch' THEN 0 ELSE 1 END, timestamp DESC",
        Limit:   100,
    })
    
    // æ ‡è®°æ•°æ®æ¥æº
    for i := range transfers {
        if transfers[i].Source == "stream" && transfers[i].CorrectedAt == nil {
            transfers[i].DataQuality = "realtime"  // å®æ—¶æ•°æ®ï¼Œå¯èƒ½æœ‰è¯¯
        } else {
            transfers[i].DataQuality = "verified"  // æ‰¹å¤„ç†éªŒè¯è¿‡çš„æ•°æ®
        }
    }
    
    return transfers, err
}
```

#### Graph Engine å¢é‡ + æ‰¹é‡åˆ†æ

```java processing/graph-engine/src/main/java/com/chainrisk/graph/service/impl/GraphSyncServiceImpl.java
@Service
@RequiredArgsConstructor
public class GraphSyncServiceImpl implements GraphSyncService {
    
    // âœ… å¢é‡åˆ†æï¼ˆKafka è§¦å‘ï¼‰
    @KafkaListener(topics = "transfers", groupId = "graph-engine")
    public void onNewTransfer(Transfer transfer) {
        log.debug("Received new transfer: {}", transfer.getTxHash());
        
        // 1. å¢é‡èšç±»
        if (shouldTriggerClustering(transfer)) {
            clusteringService.runIncrementalClustering(
                transfer.getFromAddr(), 
                transfer.getToAddr()
            );
        }
        
        // 2. å¢é‡æ ‡ç­¾ä¼ æ’­
        if (isHighRiskAddress(transfer.getFromAddr())) {
            tagPropagationService.propagateFromAddress(transfer.getFromAddr());
        }
    }
    
    // âœ… æ‰¹é‡åˆ†æï¼ˆå®šæ—¶ä»»åŠ¡ï¼‰
    @Scheduled(cron = "0 0 3 * * ?") // æ¯å¤©å‡Œæ™¨ 3 ç‚¹
    public void runBatchAnalysis() {
        log.info("Starting daily batch graph analysis");
        
        // 1. å…¨å›¾èšç±»
        clusteringService.runFullClustering();
        
        // 2. å…¨å›¾æ ‡ç­¾ä¼ æ’­
        tagPropagationService.propagateAllTags();
        
        // 3. PageRank
        graphAnalysisService.runPageRank();
        
        // 4. ç¤¾åŒºå‘ç°
        graphAnalysisService.runCommunityDetection();
    }
}
```

---

## ğŸ“Š æ•°æ®è¡¨è®¾è®¡

### PostgreSQL è¡¨ç»“æ„

```sql
-- transfers è¡¨ï¼ˆæ”¯æŒæµæ‰¹è¦†ç›–ï¼‰
CREATE TABLE transfers (
    tx_hash VARCHAR(66) PRIMARY KEY,
    block_number BIGINT NOT NULL,
    from_addr VARCHAR(42) NOT NULL,
    to_addr VARCHAR(42) NOT NULL,
    amount NUMERIC(78, 0) NOT NULL,
    token_address VARCHAR(42),
    timestamp BIGINT NOT NULL,
    network VARCHAR(20) NOT NULL DEFAULT 'ethereum',
    
    -- å…ƒæ•°æ®
    source VARCHAR(10) NOT NULL,  -- 'stream' æˆ– 'batch'
    created_at TIMESTAMP NOT NULL DEFAULT NOW(),
    corrected_at TIMESTAMP,  -- æ‰¹å¤„ç†è¦†ç›–æ—¶é—´
    
    INDEX idx_from_addr (from_addr),
    INDEX idx_to_addr (to_addr),
    INDEX idx_block_number (block_number),
    INDEX idx_source (source)
);

-- æŸ¥è¯¢ç¤ºä¾‹ï¼šä¼˜å…ˆä½¿ç”¨ batch æ•°æ®
SELECT * FROM transfers 
WHERE from_addr = '0x123...'
ORDER BY 
    CASE WHEN source = 'batch' THEN 0 ELSE 1 END,
    timestamp DESC
LIMIT 100;
```

### Neo4j å›¾ç»“æ„

```cypher
// Address èŠ‚ç‚¹
CREATE (a:Address {
    address: '0x123...',
    network: 'ethereum',
    risk_score: 0.5,
    tags: ['high_frequency', 'mixer_interaction'],
    source: 'stream',  // 'stream' æˆ– 'batch'
    first_seen: timestamp(),
    corrected_at: timestamp()  // æ‰¹å¤„ç†è¦†ç›–æ—¶é—´
})

// TRANSFER å…³ç³»
CREATE (from)-[r:TRANSFER {
    tx_hash: '0xabc...',
    amount: '1000000000000000000',
    timestamp: 1704240000,
    block_number: 18000000,
    source: 'stream',  // 'stream' æˆ– 'batch'
    corrected_at: timestamp()  // æ‰¹å¤„ç†è¦†ç›–æ—¶é—´
}]->(to)

// æŸ¥è¯¢ç¤ºä¾‹ï¼šä¼˜å…ˆä½¿ç”¨ batch æ•°æ®
MATCH (from:Address {address: $addr})-[r:TRANSFER]->(to)
RETURN from, r, to
ORDER BY 
    CASE WHEN r.source = 'batch' THEN 0 ELSE 1 END,
    r.timestamp DESC
LIMIT 100
```

---

## ğŸ¯ åº”ç”¨åœºæ™¯å¯¹æ¯”

### åœºæ™¯ 1: Transfer æ•°æ®æå–

| ç»´åº¦ | Flink æµå¤„ç† | Spark æ‰¹å¤„ç† |
|-----|-------------|-------------|
| **æ•°æ®æº** | Kafka å®æ—¶æ¶ˆæ¯ | å…¨èŠ‚ç‚¹ RPC é‡æ–°æ‰«æ |
| **è§£æé€»è¾‘** | ç®€åŒ–ç‰ˆï¼ˆå¿«é€Ÿï¼‰ | å®Œæ•´ç‰ˆï¼ˆå¤„ç†å¤æ‚åˆçº¦ï¼‰ |
| **æ•°æ®å®Œæ•´æ€§** | å¯èƒ½ä¸¢å¤± | ä¿è¯å®Œæ•´ |
| **åŒºå—é‡ç»„** | æ— æ³•å¤„ç† | ç­‰å¾…ç¡®è®¤åå¤„ç† |
| **æ–°åˆçº¦æ”¯æŒ** | éœ€è¦é‡å¯æ›´æ–° | å¯ä»¥å›å¡«å†å²æ•°æ® |
| **å»¶è¿Ÿ** | ç§’çº§ | T+1 å¤© |

**è¦†ç›–åŸå› **: æµå¤„ç†å¯èƒ½ä¸¢å¤±æ•°æ®ã€è§£æé”™è¯¯ã€åŒºå—é‡ç»„

---

### åœºæ™¯ 2: åœ°å€é£é™©è¯„åˆ†

| ç»´åº¦ | Flink æµå¤„ç† | Spark æ‰¹å¤„ç† |
|-----|-------------|-------------|
| **ç‰¹å¾å·¥ç¨‹** | çª—å£å†…ç®€å•ç‰¹å¾ | å…¨å±€å†å²ç‰¹å¾ |
| **æ¨¡å‹å¤æ‚åº¦** | è½»é‡è§„åˆ™å¼•æ“ | å¤æ‚ ML æ¨¡å‹ï¼ˆXGBoostã€GNNï¼‰ |
| **è®¡ç®—èµ„æº** | å—é™ï¼ˆå»¶è¿Ÿè¦æ±‚ï¼‰ | å……è¶³ï¼ˆå¯ç”¨å¤§é‡ CPU/GPUï¼‰ |
| **æ•°æ®å®Œæ•´æ€§** | å¯èƒ½ç¼ºå°‘å…³è”æ•°æ® | å¯ä»¥ JOIN æ‰€æœ‰å†å²è¡¨ |
| **å»¶è¿Ÿ** | ç§’çº§ | T+1 å¤© |

**è¦†ç›–åŸå› **: æ‰¹å¤„ç†å¯ä»¥è®¡ç®—å…¨å±€ç‰¹å¾ã€ä½¿ç”¨å¤æ‚æ¨¡å‹

---

### åœºæ™¯ 3: åœ°å€èšç±»ä¸æ ‡ç­¾ä¼ æ’­

| ç»´åº¦ | Graph Engine å¢é‡ | Graph Engine æ‰¹é‡ |
|-----|------------------|------------------|
| **è§¦å‘æ–¹å¼** | Kafka æ¶ˆæ¯è§¦å‘ | å®šæ—¶ä»»åŠ¡ï¼ˆæ¯æ—¥å‡Œæ™¨ï¼‰ |
| **åˆ†æèŒƒå›´** | å±€éƒ¨å­å›¾ | å…¨å›¾ |
| **ç®—æ³•å¤æ‚åº¦** | ç®€å•èšç±»ï¼ˆUnion-Findï¼‰ | PageRankã€Louvainã€ç¤¾åŒºå‘ç° |
| **è¿­ä»£è®¡ç®—** | ä¸æ”¯æŒ | æ”¯æŒï¼ˆSpark GraphXï¼‰ |
| **å»¶è¿Ÿ** | ç§’çº§ | æ¯æ—¥ |

**è¦†ç›–åŸå› **: æ‰¹é‡åˆ†æå¯ä»¥è¿è¡Œå¤æ‚å›¾ç®—æ³•ã€è·å¾—å…¨å±€è§†å›¾

---

## ğŸ“ˆ ç›‘æ§ä¸æ•°æ®è´¨é‡

### æ•°æ®å·®å¼‚æŠ¥å‘Š

```sql
-- æ¯æ—¥æµæ‰¹æ•°æ®å·®å¼‚ç»Ÿè®¡
SELECT 
    DATE(created_at) as date,
    COUNT(*) FILTER (WHERE source = 'stream') as stream_count,
    COUNT(*) FILTER (WHERE source = 'batch') as batch_count,
    COUNT(*) FILTER (WHERE corrected_at IS NOT NULL) as corrected_count,
    ROUND(100.0 * COUNT(*) FILTER (WHERE corrected_at IS NOT NULL) / 
          NULLIF(COUNT(*) FILTER (WHERE source = 'stream'), 0), 2) as correction_rate_pct
FROM transfers
WHERE created_at >= CURRENT_DATE - INTERVAL '30 days'
GROUP BY DATE(created_at)
ORDER BY date DESC;
```

### ç›‘æ§æŒ‡æ ‡

```yaml
metrics:
  # æµå¤„ç†æŒ‡æ ‡
  flink_stream:
    - kafka_lag: æ¶ˆæ¯ç§¯å‹é‡
    - processing_rate: å¤„ç†é€Ÿç‡ï¼ˆæ¡/ç§’ï¼‰
    - error_rate: è§£æé”™è¯¯ç‡
    - neo4j_write_latency: Neo4j å†™å…¥å»¶è¿Ÿ
    
  # æ‰¹å¤„ç†æŒ‡æ ‡
  spark_batch:
    - job_duration: ä»»åŠ¡æ‰§è¡Œæ—¶é•¿
    - records_processed: å¤„ç†è®°å½•æ•°
    - correction_count: ä¿®æ­£è®°å½•æ•°
    - correction_rate: ä¿®æ­£ç‡
    
  # å›¾åˆ†ææŒ‡æ ‡
  graph_engine:
    - incremental_analysis_latency: å¢é‡åˆ†æå»¶è¿Ÿ
    - batch_analysis_duration: æ‰¹é‡åˆ†ææ—¶é•¿
    - clusters_created: åˆ›å»ºçš„èšç±»æ•°
    - tags_propagated: ä¼ æ’­çš„æ ‡ç­¾æ•°
```

---

## ğŸš€ ä¼˜åŠ¿æ€»ç»“

| ç»´åº¦ | Lambda æ¶æ„ä¼˜åŠ¿ |
|-----|----------------|
| **å®æ—¶æ€§** | Flink ç›´æ¥å†™å…¥ Neo4jï¼ŒGraph Engine ç§’çº§å“åº” |
| **å‡†ç¡®æ€§** | Spark æ‰¹å¤„ç†è¦†ç›–ä¿®æ­£é”™è¯¯æ•°æ® |
| **æ•°æ®å®Œæ•´æ€§** | æ‰¹å¤„ç†ä¿è¯æœ€ç»ˆä¸€è‡´æ€§ |
| **ç³»ç»Ÿè§£è€¦** | æµæ‰¹åˆ†ç¦»ï¼ŒGraph Engine æ— éœ€åŒæ­¥æ•°æ® |
| **èµ„æºä¼˜åŒ–** | å‡å°‘ PostgreSQL æŸ¥è¯¢å‹åŠ›ï¼Œæ— é‡å¤è®¡ç®— |
| **å¯æ‰©å±•æ€§** | æµæ‰¹ç‹¬ç«‹æ‰©å±•ï¼Œäº’ä¸å½±å“ |

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [é¡¹ç›®æ€»è§ˆ](./PROJECT_OVERVIEW.md)
- [æŠ€æœ¯å†³ç­–è®°å½•](./TECH_DECISIONS.md)
- [å¼€å‘è®¡åˆ’](../development/DEVELOPMENT_PLAN.md)

---

**æœ€åæ›´æ–°**: 2026-01-03
